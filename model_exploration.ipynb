{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:27.854061Z",
     "start_time": "2024-11-16T19:25:27.852033Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we specify arbitrary constants or hiperparams"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`TEST_TRAIN_SPLIT` - test size (0.2 == 20%) <br>\n",
    "`ROWS_PER_EXAMPLE` - number of rows (key presses) per data chunk <br>\n",
    "`NUM_HIDDEN_DIMS` - TODO <br>\n",
    "`NUM_EPOCHS` - number of training epochs"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:27.897886Z",
     "start_time": "2024-11-16T19:25:27.896383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_TRAIN_SPLIT = 0.2\n",
    "ROWS_PER_EXAMPLE = 150\n",
    "NUM_HIDDEN_DIMS = 64\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "file_names = [\"datasets/key_presses (1).tsv\", \"datasets/key_presses (2).tsv\", \"datasets/key_presses (3).tsv\", \"datasets/key_presses (4).tsv\"]"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:27.953617Z",
     "start_time": "2024-11-16T19:25:27.945774Z"
    }
   },
   "source": [
    "\n",
    "# Define a simple GCN model\n",
    "class LetterGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes, num_layers=2):\n",
    "        super(LetterGNN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(pyg_nn.GCNConv(num_node_features, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(pyg_nn.GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = x.float()\n",
    "            edge_index = edge_index.long()\n",
    "            x = conv.forward(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # idk maybe use a different pool method \n",
    "        x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        # classify\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Take a look at `dataset.x size`. Size (13, 5) means there is 13 different letters in data chunk, 5 attributes for each letter."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:29.259474Z",
     "start_time": "2024-11-16T19:25:28.007017Z"
    }
   },
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import data_loader\n",
    "\n",
    "mode = data_loader.load_char_mode.DROP\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# This is a list of lists of Data objects - datasets\n",
    "# each dataset comes from a different input file - a different user\n",
    "list_of_datasets = [data_loader.load_data_object(filename, mode=mode, y=torch.tensor([i]), rows_per_example=ROWS_PER_EXAMPLE)\n",
    "                     for i, filename in enumerate(file_names)]\n",
    "\n",
    "num_features = list_of_datasets[0][0].x.shape[1]\n",
    "\n",
    "print(\"Number of examples: \", sum(len(l) for l in list_of_datasets))\n",
    "print(\"Shape of dataset.x: \", tuple(list_of_datasets[0][0].x.shape))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  72\n",
      "Shape of dataset.x:  (24, 2)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:29.277688Z",
     "start_time": "2024-11-16T19:25:29.263742Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "training_dataset_pos = []\n",
    "testing_dataset_pos = []\n",
    "training_dataset_neg = []\n",
    "testing_dataset_neg = []\n",
    "\n",
    "# !!! choose which one get the positive label\n",
    "positive_index = 2 # just arbitrary\n",
    "\n",
    "# relabel the datasets\n",
    "for i, dataset in enumerate(list_of_datasets):\n",
    "    for item in dataset:\n",
    "        if i == positive_index:\n",
    "            item.y = torch.tensor([1])\n",
    "        else:\n",
    "            item.y = torch.tensor([0])\n",
    "\n",
    "    train_items, test_items = train_test_split(dataset, test_size=TEST_TRAIN_SPLIT)\n",
    "    \n",
    "    if i == positive_index:\n",
    "        training_dataset_pos.extend(train_items)\n",
    "        testing_dataset_pos.extend(test_items)\n",
    "    else:\n",
    "        training_dataset_neg.extend(train_items)\n",
    "        testing_dataset_neg.extend(test_items)\n",
    "\n",
    "\n",
    "class SimpleGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(SimpleGraphDataset, self).__init__('.', None, None, None)\n",
    "        self.data, self.slices = self.collate(data_list)  # Collate all data objects\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.y)  # Number of graphs in the dataset\n",
    "    \n",
    "    def statistics(self) -> str:\n",
    "        class_counts = Counter(self.data.y.cpu().numpy())\n",
    "        return \" | \".join([f\"{item_class}: {count}\" for item_class, count in class_counts.items()]) + f\" | Total: {sum(class_counts.values())}\"\n",
    "        \n",
    "\n",
    "train_items = [e.to(device) for e in (training_dataset_pos + training_dataset_neg)]\n",
    "test_items = [e.to(device) for e in (testing_dataset_pos + testing_dataset_neg)]\n",
    "\n",
    "dataset = SimpleGraphDataset(train_items)\n",
    "test_dataset = SimpleGraphDataset(test_items)\n",
    "\n",
    "print(\"Train dataset statistics: \", dataset.statistics())\n",
    "print(\"Test dataset statistics:  \", test_dataset.statistics())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset statistics:  1: 14 | 0: 42 | Total: 56\n",
      "Test dataset statistics:   1: 4 | 0: 12 | Total: 16\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:36.812290Z",
     "start_time": "2024-11-16T19:25:29.317838Z"
    }
   },
   "source": [
    "# Train the model \n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "# Q: How Do we choose hidden dims size ?? !!!!!!!!!!\n",
    "\n",
    "# Define the model, loss, and optimizer\n",
    "model = LetterGNN(num_node_features=dataset.num_node_features, hidden_dim=NUM_HIDDEN_DIMS, num_classes=dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "def train(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = criterion(output, data.y)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Training over epochs\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "prev_loss = 100000\n",
    "for epoch in range(1, NUM_EPOCHS):\n",
    "    loss = train(model, data_loader)\n",
    "    if epoch % 50 == 0 or epoch == NUM_EPOCHS-1:\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "    if round(loss, 3) == 0:\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/.venvs/gnn/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 0.4093\n",
      "Epoch: 100, Loss: 0.3919\n",
      "Epoch: 150, Loss: 0.3871\n",
      "Epoch: 200, Loss: 0.4612\n",
      "Epoch: 250, Loss: 0.3164\n",
      "Epoch: 300, Loss: 0.2984\n",
      "Epoch: 350, Loss: 0.2988\n",
      "Epoch: 400, Loss: 0.2580\n",
      "Epoch: 450, Loss: 0.2650\n",
      "Epoch: 500, Loss: 0.2642\n",
      "Epoch: 550, Loss: 0.3082\n",
      "Epoch: 600, Loss: 0.1967\n",
      "Epoch: 650, Loss: 0.1937\n",
      "Epoch: 700, Loss: 0.2020\n",
      "Epoch: 750, Loss: 0.2116\n",
      "Epoch: 800, Loss: 0.1745\n",
      "Epoch: 850, Loss: 0.2496\n",
      "Epoch: 900, Loss: 0.4065\n",
      "Epoch: 950, Loss: 0.2068\n",
      "Epoch: 999, Loss: 0.1860\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:25:36.857478Z",
     "start_time": "2024-11-16T19:25:36.816546Z"
    }
   },
   "source": [
    "# test \n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in data_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        # print(f\"Correct {data.y[0]} Pred {pred[0]}\")\n",
    "        if pred != data.y:\n",
    "            print(output[0])\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "def test_acc():\n",
    "    # Test the model\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    accuracy = test(model, test_loader)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "test_acc()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5694, -3.9978], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-3.4760, -4.1096], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-3.5617, -3.0261], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Test Accuracy: 0.8125\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
