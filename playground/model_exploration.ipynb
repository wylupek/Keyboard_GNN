{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:27.396440Z",
     "start_time": "2024-11-22T17:09:27.391766Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we specify arbitrary constants or hiperparams"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`TEST_TRAIN_SPLIT` - test size (0.2 == 20%) <br>\n",
    "`ROWS_PER_EXAMPLE` - number of rows (key presses) per data chunk <br>\n",
    "`NUM_HIDDEN_DIMS` - TODO <br>\n",
    "`NUM_EPOCHS` - number of training epochs"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:27.407181Z",
     "start_time": "2024-11-22T17:09:27.405259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_TRAIN_SPLIT = 0.2\n",
    "ROWS_PER_EXAMPLE = 150\n",
    "NUM_HIDDEN_DIMS = 64\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "file_names = [\"../datasets/user1.tsv\", \"../datasets/user2.tsv\", \"../datasets/user3.tsv\", \"../datasets/user4.tsv\"]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:27.467078Z",
     "start_time": "2024-11-22T17:09:27.459255Z"
    }
   },
   "source": [
    "\n",
    "# Define a simple GCN model\n",
    "class LetterGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes, num_layers=2):\n",
    "        super(LetterGNN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(pyg_nn.GCNConv(num_node_features, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(pyg_nn.GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = x.float()\n",
    "            edge_index = edge_index.long()\n",
    "            x = conv.forward(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # idk maybe use a different pool method \n",
    "        x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        # classify\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Take a look at `dataset.x size`. Size (13, 5) means there is 13 different letters in data chunk, 5 attributes for each letter."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:28.710016Z",
     "start_time": "2024-11-22T17:09:27.517264Z"
    }
   },
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import data_loader\n",
    "\n",
    "mode = data_loader.LoadMode.DROP\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# This is a list of lists of Data objects - datasets\n",
    "# each dataset comes from a different input file - a different user\n",
    "list_of_datasets = [\n",
    "    data_loader.load_from_file(filename, mode=mode, y=torch.tensor([i]), rows_per_example=ROWS_PER_EXAMPLE)\n",
    "    for i, filename in enumerate(file_names)]\n",
    "\n",
    "num_features = list_of_datasets[0][0].x.shape[1]\n",
    "\n",
    "print(\"Number of examples: \", sum(len(l) for l in list_of_datasets))\n",
    "print(\"Shape of dataset.x: \", tuple(list_of_datasets[0][0].x.shape))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  72\n",
      "Shape of dataset.x:  (24, 2)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:28.727738Z",
     "start_time": "2024-11-22T17:09:28.715018Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "training_dataset_pos = []\n",
    "testing_dataset_pos = []\n",
    "training_dataset_neg = []\n",
    "testing_dataset_neg = []\n",
    "\n",
    "# !!! choose which one get the positive label\n",
    "positive_index = 2 # just arbitrary\n",
    "\n",
    "# relabel the datasets\n",
    "for i, dataset in enumerate(list_of_datasets):\n",
    "    for item in dataset:\n",
    "        if i == positive_index:\n",
    "            item.y = torch.tensor([1])\n",
    "        else:\n",
    "            item.y = torch.tensor([0])\n",
    "\n",
    "    train_items, test_items = train_test_split(dataset, test_size=TEST_TRAIN_SPLIT)\n",
    "    \n",
    "    if i == positive_index:\n",
    "        training_dataset_pos.extend(train_items)\n",
    "        testing_dataset_pos.extend(test_items)\n",
    "    else:\n",
    "        training_dataset_neg.extend(train_items)\n",
    "        testing_dataset_neg.extend(test_items)\n",
    "\n",
    "\n",
    "class SimpleGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(SimpleGraphDataset, self).__init__('.', None, None, None)\n",
    "        self.data, self.slices = self.collate(data_list)  # Collate all data objects\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.y)  # Number of graphs in the dataset\n",
    "    \n",
    "    def statistics(self) -> str:\n",
    "        class_counts = Counter(self.data.y.cpu().numpy())\n",
    "        return \" | \".join([f\"{item_class}: {count}\" for item_class, count in class_counts.items()]) + f\" | Total: {sum(class_counts.values())}\"\n",
    "        \n",
    "\n",
    "train_items = [e.to(device) for e in (training_dataset_pos + training_dataset_neg)]\n",
    "test_items = [e.to(device) for e in (testing_dataset_pos + testing_dataset_neg)]\n",
    "\n",
    "dataset = SimpleGraphDataset(train_items)\n",
    "test_dataset = SimpleGraphDataset(test_items)\n",
    "\n",
    "print(\"Train dataset statistics: \", dataset.statistics())\n",
    "print(\"Test dataset statistics:  \", test_dataset.statistics())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset statistics:  1: 14 | 0: 42 | Total: 56\n",
      "Test dataset statistics:   1: 4 | 0: 12 | Total: 16\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:36.281037Z",
     "start_time": "2024-11-22T17:09:28.767990Z"
    }
   },
   "source": [
    "# Train the model \n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "# Q: How Do we choose hidden dims size ?? !!!!!!!!!!\n",
    "\n",
    "# Define the model, loss, and optimizer\n",
    "model = LetterGNN(num_node_features=dataset.num_node_features, hidden_dim=NUM_HIDDEN_DIMS, num_classes=dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "def train(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = criterion(output, data.y)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Training over epochs\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "prev_loss = 100000\n",
    "for epoch in range(1, NUM_EPOCHS):\n",
    "    loss = train(model, data_loader)\n",
    "    if epoch % 50 == 0 or epoch == NUM_EPOCHS-1:\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "    if round(loss, 3) == 0:\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/.venvs/gnn/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 0.4527\n",
      "Epoch: 100, Loss: 0.4031\n",
      "Epoch: 150, Loss: 0.3313\n",
      "Epoch: 200, Loss: 0.3245\n",
      "Epoch: 250, Loss: 0.2954\n",
      "Epoch: 300, Loss: 0.2870\n",
      "Epoch: 350, Loss: 0.5434\n",
      "Epoch: 400, Loss: 0.2684\n",
      "Epoch: 450, Loss: 0.2408\n",
      "Epoch: 500, Loss: 0.2272\n",
      "Epoch: 550, Loss: 0.2435\n",
      "Epoch: 600, Loss: 0.2282\n",
      "Epoch: 650, Loss: 0.1972\n",
      "Epoch: 700, Loss: 0.2583\n",
      "Epoch: 750, Loss: 0.2294\n",
      "Epoch: 800, Loss: 0.1800\n",
      "Epoch: 850, Loss: 0.2123\n",
      "Epoch: 900, Loss: 0.2493\n",
      "Epoch: 950, Loss: 0.1799\n",
      "Epoch: 999, Loss: 0.2048\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:36.329156Z",
     "start_time": "2024-11-22T17:09:36.292699Z"
    }
   },
   "source": [
    "# test \n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in data_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        # print(f\"Correct {data.y[0]} Pred {pred[0]}\")\n",
    "        if pred != data.y:\n",
    "            print(output[0])\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "def test_acc():\n",
    "    # Test the model\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    accuracy = test(model, test_loader)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "test_acc()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-14.8833, -15.0566], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Test Accuracy: 0.9375\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:36.342007Z",
     "start_time": "2024-11-22T17:09:36.339662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_PATH = '../models/experimental.pth'\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/experimental.pth\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:36.413587Z",
     "start_time": "2024-11-22T17:09:36.401450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model (for inference)\n",
    "loaded_model = LetterGNN(num_node_features=dataset.num_node_features, hidden_dim=NUM_HIDDEN_DIMS, num_classes=dataset.num_classes).to(device)\n",
    "loaded_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded for inference.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46986/1325433200.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:36.496293Z",
     "start_time": "2024-11-22T17:09:36.460960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming you have a new data loader for inference data\n",
    "inference_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Inference loop\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for data in inference_loader:\n",
    "        data = data.to(device)\n",
    "        output = loaded_model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.argmax(dim=1)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:09:36.544807Z",
     "start_time": "2024-11-22T17:09:36.510772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in data_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        # print(f\"Correct {data.y[0]} Pred {pred[0]}\")\n",
    "        if pred != data.y:\n",
    "            print(output[0])\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "def test_acc():\n",
    "    # Test the model\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    accuracy = test(loaded_model, test_loader)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "test_acc()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-14.8833, -15.0566], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Test Accuracy: 0.9375\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
